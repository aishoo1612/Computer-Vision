{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenization using PyTorch",
      "provenance": [],
      "authorship_tag": "ABX9TyPLqs86RS1dmDi0dAjxsGJQ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4mM6LRC-5Pm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "0154897f-df19-4269-e522-dbe99e41648c"
      },
      "source": [
        "!pip install keras\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.2)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k1iFaL6yAS25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import torch \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_itPIMpjAiv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerModel(nn.Module):\n",
        "  def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout = 0.5):\n",
        "    super(TransformerModel, self).__init__()\n",
        "    self.model_type = 'Transformer'\n",
        "    self.src_mask = None \n",
        "    self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
        "    encoder_layers = TransformerEncoderLayer(ninp, nhead, nhid, dropout, activation='relu')\n",
        "    \n",
        "    \n",
        "    self.Transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
        "    self.encoder = nn.Embedding(ntoken, ninp)\n",
        "    self.ninp = ninp \n",
        "    self.decoder = nn.Linear(ninp. ntoken)\n",
        "\n",
        "    self.init_weights()\n",
        "\n",
        "  def _generate_square_subsequent_mask(self, sz):\n",
        "    mask = (torch.triu(torch.ones(sz,sz)) == 1).transpose(0,1)\n",
        "    mask = mask.float().masked_fill(mask ==0, float ('inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask \n",
        "  \n",
        "  def init_weights(self):\n",
        "    initrange = 0.1 \n",
        "    self.encoder.weight.data.uniform_(-0.1, 0.1)\n",
        "    self.decoder.bias.data.zero_()\n",
        "    self.decoder.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "  def forward(self, src):\n",
        "    if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
        "      device = src.device\n",
        "      mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
        "\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    }
  ]
}